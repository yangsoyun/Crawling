{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading /images/branding/googlelogo/2x/googlelogo_color_92x30dp.png: Invalid URL '/images/branding/googlelogo/2x/googlelogo_color_92x30dp.png': No scheme supplied. Perhaps you meant https:///images/branding/googlelogo/2x/googlelogo_color_92x30dp.png?\n",
      "Error downloading /tia/tia.png: Invalid URL '/tia/tia.png': No scheme supplied. Perhaps you meant https:///tia/tia.png?\n",
      "Error downloading None: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?\n",
      "Error downloading None: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?\n",
      "Error downloading None: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?\n",
      "Error downloading None: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?\n",
      "Error downloading None: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?\n",
      "Error downloading None: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?\n",
      "image_13.jpg 다운로드 완료\n",
      "image_14.jpg 다운로드 완료\n",
      "image_15.jpg 다운로드 완료\n",
      "image_16.jpg 다운로드 완료\n",
      "image_17.jpg 다운로드 완료\n",
      "image_179.jpg 다운로드 완료\n",
      "image_181.jpg 다운로드 완료\n",
      "image_183.jpg 다운로드 완료\n",
      "image_185.jpg 다운로드 완료\n",
      "image_187.jpg 다운로드 완료\n",
      "image_189.jpg 다운로드 완료\n",
      "image_191.jpg 다운로드 완료\n",
      "image_193.jpg 다운로드 완료\n",
      "image_195.jpg 다운로드 완료\n",
      "image_197.jpg 다운로드 완료\n",
      "image_199.jpg 다운로드 완료\n",
      "image_201.jpg 다운로드 완료\n",
      "image_203.jpg 다운로드 완료\n",
      "image_205.jpg 다운로드 완료\n",
      "image_207.jpg 다운로드 완료\n",
      "image_209.jpg 다운로드 완료\n",
      "image_211.jpg 다운로드 완료\n",
      "image_213.jpg 다운로드 완료\n",
      "image_215.jpg 다운로드 완료\n",
      "image_217.jpg 다운로드 완료\n",
      "image_219.jpg 다운로드 완료\n",
      "image_220.jpg 다운로드 완료\n",
      "image_221.jpg 다운로드 완료\n",
      "image_222.jpg 다운로드 완료\n",
      "image_224.jpg 다운로드 완료\n",
      "image_226.jpg 다운로드 완료\n",
      "image_228.jpg 다운로드 완료\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# ChromeDriver 경로 설정\n",
    "chromedriver_path = os.path.join(os.getcwd(), \"chromedriver.exe\")\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# 구글 이미지 검색 URL로 이동\n",
    "query = \"beautiful roses\"\n",
    "url = f\"https://www.google.com/search?q={query}&tbm=isch\"\n",
    "driver.get(url)\n",
    "\n",
    "# 저장 디렉토리 설정\n",
    "save_dir = \"downloaded_images3\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 스크롤 다운 반복\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# BeautifulSoup로 HTML 파싱\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "img_tags = soup.find_all('img')\n",
    "\n",
    "# 이미지 다운로드\n",
    "for idx, img_tag in enumerate(img_tags):\n",
    "    img_url = img_tag.get('src')\n",
    "    \n",
    "    # 데이터 URL 건너뛰기\n",
    "    if img_url and img_url.startswith('data:image/'):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        img_data = requests.get(img_url).content\n",
    "        filename = f\"image_{idx}.jpg\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(img_data)\n",
    "        print(f\"{filename} 다운로드 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {img_url}: {e}\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 크롬 드라이버 경로 설정\n",
    "chromedriver_path = os.path.join(os.getcwd(), \"chromedriver.exe\")  # 크롬 드라이버 경로를 입력해주세요.\n",
    "\n",
    "# 크롬 드라이버 서비스 객체 생성\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# 웹 드라이버 객체 생성\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# 크롤링할 URL\n",
    "url = 'https://www.boannews.com/media/t_list.asp?mkind=0'\n",
    "\n",
    "# 웹 페이지 열기\n",
    "driver.get(url)\n",
    "\n",
    "# 뉴스 데이터를 저장할 리스트\n",
    "news_data = []\n",
    "\n",
    "# 크롤링할 페이지 수\n",
    "num_pages = 2  # 원하는 페이지 수를 입력해주세요.\n",
    "\n",
    "for page in range(1, num_pages + 1):\n",
    "    # 현재 페이지의 HTML 소스 코드 가져오기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 뉴스 목록 찾기\n",
    "    news_list = soup.select('.news_list a:not(.news_content)')\n",
    "\n",
    "    for news in news_list:\n",
    "        # 뉴스 링크 추출\n",
    "        news_url = 'https://www.boannews.com' + news['href']\n",
    "\n",
    "        # 새 탭에서 뉴스 링크 열기\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        driver.get(news_url)\n",
    "\n",
    "        # 뉴스 내용 크롤링\n",
    "        news_html = driver.page_source\n",
    "        news_soup = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "        # 뉴스 제목\n",
    "        news_title = news_soup.select_one('#news_title02 h1').text.strip()\n",
    "\n",
    "        # 뉴스 날짜\n",
    "        news_date = ':'.join(news_soup.select_one('#news_util01').text.strip().split(':')[1:]).strip()\n",
    "\n",
    "        # 뉴스 내용\n",
    "        news_content = news_soup.select_one('#news_content').text.strip()\n",
    "\n",
    "        # 크롤링한 데이터를 리스트에 추가\n",
    "        news_data.append([news_title, news_date, news_content])\n",
    "\n",
    "        # 탭 닫기\n",
    "        driver.close()\n",
    "\n",
    "        # 원래 탭으로 돌아가기\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    # 다음 페이지로 이동\n",
    "    next_page_url = f'https://www.boannews.com/media/t_list.asp?Page={page+1}&kind=0'\n",
    "    driver.get(next_page_url)\n",
    "    time.sleep(2)  # 2초 대기 (페이지 로딩을 위해)\n",
    "\n",
    "# 웹 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "# CSV 파일로 저장\n",
    "with open('news_data3.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Date', 'Content'])\n",
    "    writer.writerows(news_data)\n",
    "\n",
    "print('크롤링이 완료되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 크롬 드라이버 경로 설정\n",
    "chromedriver_path = os.path.join(os.getcwd(), \"chromedriver.exe\")  # 크롬 드라이버 경로를 입력해주세요.\n",
    "\n",
    "# 크롬 드라이버 서비스 객체 생성\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# 웹 드라이버 객체 생성\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# 크롤링할 URL\n",
    "url = 'https://www.boannews.com/search/news_total.asp?search=title&find=%BE%CF%C8%A3'\n",
    "\n",
    "# 웹 페이지 열기\n",
    "driver.get(url)\n",
    "\n",
    "# 뉴스 데이터를 저장할 리스트\n",
    "news_data = []\n",
    "\n",
    "# 크롤링할 페이지 수\n",
    "num_pages = 2  # 원하는 페이지 수를 입력해주세요.\n",
    "\n",
    "for page in range(1, num_pages + 1):\n",
    "    # 현재 페이지의 HTML 소스 코드 가져오기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 뉴스 목록 찾기\n",
    "    news_list = soup.select('.news_list a:not(.news_content)')\n",
    "\n",
    "    for news in news_list:\n",
    "        # 뉴스 링크 추출\n",
    "        news_url = 'https://www.boannews.com' + news['href']\n",
    "\n",
    "        # 새 탭에서 뉴스 링크 열기\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        driver.get(news_url)\n",
    "\n",
    "        # 뉴스 내용 크롤링\n",
    "        news_html = driver.page_source\n",
    "        news_soup = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "        # 뉴스 제목\n",
    "        news_title = news_soup.select_one('#news_title02 h1').text.strip()\n",
    "\n",
    "        # 뉴스 날짜\n",
    "        news_date = ':'.join(news_soup.select_one('#news_util01').text.strip().split(':')[1:]).strip()\n",
    "\n",
    "        # 뉴스 내용\n",
    "        news_content = news_soup.select_one('#news_content').text.strip()\n",
    "\n",
    "        # 크롤링한 데이터를 리스트에 추가\n",
    "        news_data.append([news_title, news_date, news_content])\n",
    "\n",
    "        # 탭 닫기\n",
    "        driver.close()\n",
    "\n",
    "        # 원래 탭으로 돌아가기\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    # 다음 페이지로 이동\n",
    "    next_page_url = f'https://www.boannews.com/search/news_total.asp?Page={page+1}&search=title&find=%BE%CF%C8%A3'\n",
    "    driver.get(next_page_url)\n",
    "    time.sleep(2)  # 2초 대기 (페이지 로딩을 위해)\n",
    "\n",
    "# 웹 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "# CSV 파일로 저장\n",
    "with open('news_data8.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Date', 'Content'])\n",
    "    writer.writerows(news_data)\n",
    "\n",
    "print('크롤링이 완료되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "댓글을 단 링크 목록:\n",
      "https://creamerburger.tistory.com/33\n",
      "https://creamerburger.tistory.com/32\n",
      "https://creamerburger.tistory.com/31\n",
      "https://creamerburger.tistory.com/30\n",
      "https://creamerburger.tistory.com/29\n",
      "https://creamerburger.tistory.com/28\n",
      "https://creamerburger.tistory.com/27\n",
      "https://creamerburger.tistory.com/26\n",
      "https://creamerburger.tistory.com/25\n",
      "https://creamerburger.tistory.com/24\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 크롬 드라이버 경로 설정\n",
    "chromedriver_path = os.path.join(os.getcwd(), \"chromedriver.exe\")  # 크롬 드라이버 경로를 입력해주세요.\n",
    "\n",
    "# 크롬 드라이버 서비스 객체 생성\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# 웹 드라이버 객체 생성\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# 블로그 URL\n",
    "blog_url = 'https://creamerburger.tistory.com'\n",
    "\n",
    "# 웹 페이지 열기\n",
    "driver.get(blog_url)\n",
    "\n",
    "# 게시물 링크 추출\n",
    "post_links = driver.find_elements(By.CSS_SELECTOR, '.list_content .link_post')\n",
    "\n",
    "# 댓글을 단 링크 목록\n",
    "commented_links = []\n",
    "\n",
    "for link in post_links:\n",
    "    post_url = link.get_attribute('href')\n",
    "    \n",
    "    # 새 탭에서 게시물 링크 열기\n",
    "    driver.execute_script(\"window.open('');\")\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    driver.get(post_url)\n",
    "    \n",
    "    # 댓글 작성 필드 찾기\n",
    "    name_input = driver.find_element(By.CSS_SELECTOR, 'input[placeholder=\"이름\"]')\n",
    "    password_input = driver.find_element(By.CSS_SELECTOR, 'input[placeholder=\"비밀번호\"]')\n",
    "    comment_textarea = driver.find_element(By.CSS_SELECTOR, 'textarea[placeholder=\"댓글을 달아주세요 :)\"]')\n",
    "    submit_button = driver.find_element(By.CSS_SELECTOR, '.commentWrite input[type=\"submit\"]')\n",
    "    \n",
    "    # 댓글 작성\n",
    "    name_input.send_keys(\"융보공 동아리부원 양\")\n",
    "    password_input.send_keys(\"1234\")\n",
    "    comment_textarea.send_keys(\"예지님 열일하시네요!ㅎ.ㅎ 힘내세요 화이팅~~ 오늘 활동 재밌어욤^.^\")\n",
    "    submit_button.click()\n",
    "    \n",
    "    time.sleep(2)  # 댓글 작성 후 잠시 대기\n",
    "    \n",
    "    # 댓글을 단 링크 목록에 추가\n",
    "    commented_links.append(post_url)\n",
    "    \n",
    "    # 탭 닫기\n",
    "    driver.close()\n",
    "    \n",
    "    # 원래 탭으로 돌아가기\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "# 웹 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 댓글을 단 링크 목록 출력\n",
    "print(\"댓글을 단 링크 목록:\")\n",
    "for link in commented_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
